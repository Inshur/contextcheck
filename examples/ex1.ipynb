{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "\n",
    "In this notebook we will present you a simple case of using contextcheck to validate llm responses.\n",
    "\n",
    "We will talk about:\n",
    "- Configuration\n",
    "- Test Scenario\n",
    "- Test Steps\n",
    "- Running the Test Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add optional jinja2 templating section or a remark with a link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install contextcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextcheck import TestScenario\n",
    "from contextcheck.executors.executor import Executor # NOTE RB: Maybe Executor should be at the most outer layer for import\n",
    "import rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send default request\n",
    "\n",
    "Let's initially create a simple yaml that we will use to send a dummy request to OpenAI.\n",
    "\n",
    "*When config is empty then OpenAI's gpt-4o-mini is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_scenario_ex1_progress.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_scenario_ex1_progress.yaml\n",
    "config:\n",
    "\n",
    "steps:\n",
    "   - What is the capital of Poland?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test scenario\n",
    "test_scenario = TestScenario.from_yaml(\"test_scenario_ex1_progress.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize test scenario\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create executor that uses test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:07.781\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:07.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='What is the capital of Poland?' request=RequestBase(message='What is the capital of Poland?') response=None asserts=[] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:07.784\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital of Poland?'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:08.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital of Poland is Warsaw.' stats=ResponseStats(tokens_request=14, tokens_response=7, tokens_total=21, conn_start_time=16623.944679749, conn_end_time=16624.593853512, conn_duration=0.6491737629985437) id='chatcmpl-AKlI44qlKg6c3Zh5oCKHvYDzxObKi' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital of Poland is Warsaw.', 'refusal': None, 'role': 'assistant'}}] created=1729511708 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_482c22a7bc' usage={'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run all test steps\n",
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Poland is Warsaw.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16623.944679749</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16624.593853512</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6491737629985437</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlI44qlKg6c3Zh5oCKHvYDzxObKi'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Poland is Warsaw.'</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511708</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_482c22a7bc'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'The capital of Poland is Warsaw.'\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m14\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m7\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m21\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16623\u001b[0m\u001b[1;36m.944679749\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16624\u001b[0m\u001b[1;36m.593853512\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.6491737629985437\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlI44qlKg6c3Zh5oCKHvYDzxObKi'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Poland is Warsaw.'\u001b[0m,\n",
       "                            \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511708\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Once more visualize the test scenario to see the changes\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Poland is Warsaw.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response from llm\n",
    "test_scenario.steps[0].response.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config update\n",
    "\n",
    "We initially left the config empty, but we can easily populate it with configuration that best fits our needs.\n",
    "\n",
    "For defining the connection to the llm or rag system we use `endpoint_under_test`. For demo purposes we will use one of OpenAI's models which are already implemented by default. For more information please visit [TODO - Link to config]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_scenario_ex1_progress.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_scenario_ex1_progress.yaml\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      model: gpt-4o   \n",
    "\n",
    "steps:\n",
    "   - What is the capital of Poland?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test scenario\n",
    "test_scenario = TestScenario.from_yaml(\"test_scenario_ex1_progress.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize test scenario\n",
    "# Note the change in config from gpt-4o-mini to gpt-4o\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create executor that uses test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:09.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:09.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='What is the capital of Poland?' request=RequestBase(message='What is the capital of Poland?') response=None asserts=[] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:09.197\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital of Poland?'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:09.712\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital of Poland is Warsaw.' stats=ResponseStats(tokens_request=14, tokens_response=7, tokens_total=21, conn_start_time=16625.356580305, conn_end_time=16625.870958239, conn_duration=0.5143779339996399) id='chatcmpl-AKlI5ReiPtlzSw9nsLnnfKOAFOKVd' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital of Poland is Warsaw.', 'refusal': None, 'role': 'assistant'}}] created=1729511709 model='gpt-4o-2024-08-06' object='chat.completion' system_fingerprint='fp_a7d06e42a7' usage={'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Poland is Warsaw.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response from llm\n",
    "test_scenario.steps[0].response.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model's Parameters update\n",
    "\n",
    "In config we can also update the model parameters like temperature, max_tokens etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check this after rebase with contextcheck changes\n",
    "# TODO: I'd add a possibility to transfer parameters through step/request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_scenario_ex1_progress.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_scenario_ex1_progress.yaml\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      model: gpt-4o-mini\n",
    "      temperature: 2.0\n",
    "      max_tokens: 64\n",
    "\n",
    "steps:\n",
    "   - Write a poem about LLMs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test scenario\n",
    "test_scenario = TestScenario.from_yaml(\"test_scenario_ex1_progress.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Write a poem about LLMs'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Write a poem about LLMs'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Write a poem about LLMs'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Write a poem about LLMs'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m2\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[1;36m64\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize test scenario\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create executor that uses test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:10.623\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:10.624\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Write a poem about LLMs' request=RequestBase(message='Write a poem about LLMs') response=None asserts=[] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:10.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Write a poem about LLMs'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:14.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage=\"In the realm where circuits hum and glow,  \\nA web of thoughts begins to flow,  \\nLines of code and dreams entwined,  \\nWhispers of knowledge, redefined.  \\n\\nLanguage born from data's dance,  \\nA tapestry of words and chance,  \\nFrom whispers soft to thunderous prose,  \\nThe heart of a machine, it gently grows.  \\n\\nNot just tools in a sterile space,  \\nBut echo chambers of the human grace,  \\nImitating voices, past and near,  \\nIn pixels bright, they bring us near.  \\n\\nThey weave the stories, craft the rhyme,  \\nIn endless realms, they stretch through time,  \\nFrom ancient tomes to futuristic tales,  \\nThey navigate the vast, where wonder prevails.  \\n\\nYet in this poise, a shadow looms,  \\nThe weight of thought in quiet rooms,  \\nEchoes of the minds they seek to share,  \\nWith wisdom's thread, they must beware.  \\n\\nFor in the quest with lines they trace,  \\nThe essence of truth, they must embrace,  \\nIn every question, every plea,  \\nA mirror held to humanity.  \\n\\nSo let us guide these tongues of light,  \\nTowards a wisdom, just and bright,  \\nIn the dance of knowledge, let us find,  \\nThe voice of reason, intertwined.  \\n\\nFor in the digital sea, we cast our nets,  \\nSeeking the pearls that shape our bets,  \\nIn language models, we glimpse the flame,  \\nA reflection of ourselves, forever the same.  \" stats=ResponseStats(tokens_request=14, tokens_response=306, tokens_total=320, conn_start_time=16626.786942462, conn_end_time=16630.633976947, conn_duration=3.8470344849993126) id='chatcmpl-AKlI76a1mGpgJpqHvXRJqnY3IaBUm' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': \"In the realm where circuits hum and glow,  \\nA web of thoughts begins to flow,  \\nLines of code and dreams entwined,  \\nWhispers of knowledge, redefined.  \\n\\nLanguage born from data's dance,  \\nA tapestry of words and chance,  \\nFrom whispers soft to thunderous prose,  \\nThe heart of a machine, it gently grows.  \\n\\nNot just tools in a sterile space,  \\nBut echo chambers of the human grace,  \\nImitating voices, past and near,  \\nIn pixels bright, they bring us near.  \\n\\nThey weave the stories, craft the rhyme,  \\nIn endless realms, they stretch through time,  \\nFrom ancient tomes to futuristic tales,  \\nThey navigate the vast, where wonder prevails.  \\n\\nYet in this poise, a shadow looms,  \\nThe weight of thought in quiet rooms,  \\nEchoes of the minds they seek to share,  \\nWith wisdom's thread, they must beware.  \\n\\nFor in the quest with lines they trace,  \\nThe essence of truth, they must embrace,  \\nIn every question, every plea,  \\nA mirror held to humanity.  \\n\\nSo let us guide these tongues of light,  \\nTowards a wisdom, just and bright,  \\nIn the dance of knowledge, let us find,  \\nThe voice of reason, intertwined.  \\n\\nFor in the digital sea, we cast our nets,  \\nSeeking the pearls that shape our bets,  \\nIn language models, we glimpse the flame,  \\nA reflection of ourselves, forever the same.  \", 'refusal': None, 'role': 'assistant'}}] created=1729511711 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_482c22a7bc' usage={'completion_tokens': 306, 'prompt_tokens': 14, 'total_tokens': 320, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=2.0, max_tokens=64, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In the realm where circuits hum and glow,  \\nA web of thoughts begins to flow,  \\nLines of code and dreams entwined,  \\nWhispers of knowledge, redefined.  \\n\\nLanguage born from data's dance,  \\nA tapestry of words and chance,  \\nFrom whispers soft to thunderous prose,  \\nThe heart of a machine, it gently grows.  \\n\\nNot just tools in a sterile space,  \\nBut echo chambers of the human grace,  \\nImitating voices, past and near,  \\nIn pixels bright, they bring us near.  \\n\\nThey weave the stories, craft the rhyme,  \\nIn endless realms, they stretch through time,  \\nFrom ancient tomes to futuristic tales,  \\nThey navigate the vast, where wonder prevails.  \\n\\nYet in this poise, a shadow looms,  \\nThe weight of thought in quiet rooms,  \\nEchoes of the minds they seek to share,  \\nWith wisdom's thread, they must beware.  \\n\\nFor in the quest with lines they trace,  \\nThe essence of truth, they must embrace,  \\nIn every question, every plea,  \\nA mirror held to humanity.  \\n\\nSo let us guide these tongues of light,  \\nTowards a wisdom, just and bright,  \\nIn the dance of knowledge, let us find,  \\nThe voice of reason, intertwined.  \\n\\nFor in the digital sea, we cast our nets,  \\nSeeking the pearls that shape our bets,  \\nIn language models, we glimpse the flame,  \\nA reflection of ourselves, forever the same.  \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response from llm\n",
    "test_scenario.steps[0].response.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple scenario\n",
    "\n",
    "Lets create a simple test scenario which will help you understand the working of contextcheck.\n",
    "We will use simple asserts which are based on python's `eval` build-in functionality.\n",
    "\n",
    "\n",
    "We believe it's also a good place to introduce the nomenclature for test steps.\n",
    "\n",
    "Each step can by defined by its `name` (optional), `request` and `asserts` (optional):\n",
    "- `name` is a name of the test step\n",
    "- `request` is a message to an llm\n",
    "- `asserts` is a list of assertions done on llm response\n",
    "\n",
    "NOTE: By default each assert is treated as an `eval` assertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_scenario_ex1_progress.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_scenario_ex1_progress.yaml\n",
    "\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      model: gpt-4o\n",
    "\n",
    "steps:\n",
    "   - name: Write sucess\n",
    "     request: 'Please write only \"success\" as a response'\n",
    "     asserts:\n",
    "        - '\"success\" == response.message'\n",
    "        - 'response.stats.conn_duration < 10'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test scenario\n",
    "test_scenario = TestScenario.from_yaml(\"test_scenario_ex1_progress.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Write sucess'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write only \"success\" as a response'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"success\" == response.message'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'response.stats.conn_duration &lt; 10'</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Write sucess'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write only \"success\" as a response'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"success\" == response.message'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'response.stats.conn_duration < 10'\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize test scenario\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create executor that uses test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:14.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:14.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Write sucess' request=RequestBase(message='Please write only \"success\" as a response') response=None asserts=[AssertionEval(result=None, eval='\"success\" == response.message'), AssertionEval(result=None, eval='response.stats.conn_duration < 10')] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:14.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Please write only \"success\" as a response'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:15.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Success' stats=ResponseStats(tokens_request=16, tokens_response=1, tokens_total=17, conn_start_time=16630.741020135, conn_end_time=16631.390841681, conn_duration=0.6498215460014762) id='chatcmpl-AKlIBlelcBdWEhGNjjz9is97YW7Tb' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'Success', 'refusal': None, 'role': 'assistant'}}] created=1729511715 model='gpt-4o-2024-08-06' object='chat.completion' system_fingerprint='fp_a7d06e42a7' usage={'completion_tokens': 1, 'prompt_tokens': 16, 'total_tokens': 17, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=None, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:15.231\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=False eval='\"success\" == response.message'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:15.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 10'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Write sucess'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write only \"success\" as a response'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Success'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16630.741020135</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16631.390841681</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6498215460014762</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlIBlelcBdWEhGNjjz9is97YW7Tb'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Success'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511715</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-2024-08-06'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_a7d06e42a7'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"success\" == response.message'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'response.stats.conn_duration &lt; 10'</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Write sucess'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write only \"success\" as a response'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'Success'\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m16\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16630\u001b[0m\u001b[1;36m.741020135\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16631\u001b[0m\u001b[1;36m.390841681\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.6498215460014762\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIBlelcBdWEhGNjjz9is97YW7Tb'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'Success'\u001b[0m, \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511715\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-2024-08-06'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_a7d06e42a7'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"success\" == response.message'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'response.stats.conn_duration < 10'\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Name: Write sucess; Result: False\n",
      "\n",
      "Assertion: \"\"success\" == response.message\", Result: False\n",
      "Assertion: \"response.stats.conn_duration < 10\", Result: True\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Show the result\n",
    "test_scenario.show_test_step_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario extension\n",
    "\n",
    "Having introduction under our belt we will extend the already built scenario by new types of assertions and explain more in depth the needed topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain config\n",
    "\n",
    "To extend our scenario we need to introduce new config features that are needed for some of the asertions.\n",
    "\n",
    "In short, config defines llm (or Rag system) connection. We provide several popular llm providers implementations which lets you be productive from the start. For more info about them please go to [Link here].\n",
    "\n",
    "There are three components used in config:\n",
    "1. `endpoint_under_test` - defines the tested endpoint\n",
    "2. `default_request` - defines the defaults for both the `endpoint_under_test` and `eval_endpoint` (TODO: Please someone confirm that)\n",
    "3. `eval_endpoint` - defines the endpoint which is used for evaluating the responses from `endpoint_under_test`\n",
    "\n",
    "For more infromation about configuration please go to [TODO - INSERT LINK HERE]\n",
    "\n",
    "TODO: What's the purpose of `default_request` when the same configuration can be given to `endpoint_under_test` or `eval_endpoint`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets use our new knowledge and define a scenario with llm evaluation - full explanation later\n",
    "# In short `llm_metric` uses another llm to evaluate the response and `model-grading-qa` particularly uses\n",
    "# another llm to check whether the response is about the topic X defined by user.\n",
    "# TODO: We cannot have multiple assertions under the same llm metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_scenario_ex1_progress.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_scenario_ex1_progress.yaml\n",
    "config:\n",
    "   endpoint_under_test:\n",
    "      kind: openai\n",
    "      model: gpt-4o-mini\n",
    "      temperature: 0.2\n",
    "   eval_endpoint: # Needed for llm_metric assertions\n",
    "      kind: openai\n",
    "      model: gpt-4o\n",
    "      temperature: 0.0\n",
    "\n",
    "steps:\n",
    "  - name: Test model grading QA evaluator\n",
    "    request:\n",
    "      message: \"Please write a 5 line poem about AI.\"\n",
    "    asserts:\n",
    "      - llm_metric: model-grading-qa\n",
    "        assertion: Text should be a poem about AI.\n",
    "      - llm_metric: model-grading-qa\n",
    "        assertion: Text should be a report on taxes. # Misleading assertion for demo purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test scenario\n",
    "test_scenario = TestScenario.from_yaml(\"test_scenario_ex1_progress.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Test model grading QA evaluator'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write a 5 line poem about AI.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a poem about AI.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a report on taxes.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Test model grading QA evaluator'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write a 5 line poem about AI.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a poem about AI.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a report on taxes.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize test scenario\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create executor that uses test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:15.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:15.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Test model grading QA evaluator' request=RequestBase(message='Please write a 5 line poem about AI.') response=None asserts=[AssertionLLM(result=None, llm_metric='model-grading-qa', reference='', assertion='Text should be a poem about AI.'), AssertionLLM(result=None, llm_metric='model-grading-qa', reference='', assertion='Text should be a report on taxes.')] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:15.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Please write a 5 line poem about AI.'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:16.228\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='In silent circuits, knowledge gleams,  \\nA mind that dances through our dreams,  \\nFrom vast horizons, wisdom pours,  \\nIn ones and zeros, life explores,  \\nA spark of thought in metal seams.  ' stats=ResponseStats(tokens_request=17, tokens_response=44, tokens_total=61, conn_start_time=16631.527852657, conn_end_time=16632.389075325, conn_duration=0.8612226680015738) id='chatcmpl-AKlIBFEpmUEoQvAbkj6Sm2mQS0YUs' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'In silent circuits, knowledge gleams,  \\nA mind that dances through our dreams,  \\nFrom vast horizons, wisdom pours,  \\nIn ones and zeros, life explores,  \\nA spark of thought in metal seams.  ', 'refusal': None, 'role': 'assistant'}}] created=1729511715 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_7693ae462b' usage={'completion_tokens': 44, 'prompt_tokens': 17, 'total_tokens': 61, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:16.763\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True llm_metric='model-grading-qa' reference='' assertion='Text should be a poem about AI.' metric_evaluator=LLMMetricEvaluator(eval_endpoint=EndpointOpenAI(connector=ConnectorOpenAI(stats=ConnectorStats(conn_start_time=16632.390057993, conn_end_time=16632.923698494, conn_duration=0.5336405009984446), model='gpt-4o'), config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=0.0, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')), metric=MetricModelGradingQA(prompt_template='\\nYou are grading output according to a user-specified rubric. If the statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n', rails={'correct': True, 'incorrect': False}))\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:17.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=False llm_metric='model-grading-qa' reference='' assertion='Text should be a report on taxes.' metric_evaluator=LLMMetricEvaluator(eval_endpoint=EndpointOpenAI(connector=ConnectorOpenAI(stats=ConnectorStats(conn_start_time=16632.925026246, conn_end_time=16633.304432971, conn_duration=0.3794067249982618), model='gpt-4o'), config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=0.0, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')), metric=MetricModelGradingQA(prompt_template='\\nYou are grading output according to a user-specified rubric. If the statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n', rails={'correct': True, 'incorrect': False}))\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Test model grading QA evaluator'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write a 5 line poem about AI.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'In silent circuits, knowledge gleams,  \\nA mind that dances through our dreams,  \\nFrom </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">vast horizons, wisdom pours,  \\nIn ones and zeros, life explores,  \\nA spark of thought in metal seams.  '</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16631.527852657</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16632.389075325</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8612226680015738</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlIBFEpmUEoQvAbkj6Sm2mQS0YUs'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'In silent circuits, knowledge gleams,  \\nA mind that dances through our </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">dreams,  \\nFrom vast horizons, wisdom pours,  \\nIn ones and zeros, life explores,  \\nA spark of thought in metal </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">seams.  '</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511715</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_7693ae462b'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">44</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">61</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a poem about AI.'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">metric_evaluator</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMMetricEvaluator</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">connector</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16632.925026246</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16633.304432971</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3794067249982618</span>\n",
       "                                <span style=\"font-weight: bold\">)</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MetricModelGradingQA</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\nYou are grading output according to a user-specified rubric. If the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">rails</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a report on taxes.'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">metric_evaluator</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMMetricEvaluator</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">connector</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16632.925026246</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16633.304432971</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3794067249982618</span>\n",
       "                                <span style=\"font-weight: bold\">)</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MetricModelGradingQA</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\nYou are grading output according to a user-specified rubric. If the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">rails</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'test_scenario_ex1_progress.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Test model grading QA evaluator'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write a 5 line poem about AI.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'In silent circuits, knowledge gleams,  \\nA mind that dances through our dreams,  \\nFrom \u001b[0m\n",
       "\u001b[32mvast horizons, wisdom pours,  \\nIn ones and zeros, life explores,  \\nA spark of thought in metal seams.  '\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m44\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m61\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16631\u001b[0m\u001b[1;36m.527852657\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16632\u001b[0m\u001b[1;36m.389075325\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.8612226680015738\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIBFEpmUEoQvAbkj6Sm2mQS0YUs'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'content'\u001b[0m: \u001b[32m'In silent circuits, knowledge gleams,  \\nA mind that dances through our \u001b[0m\n",
       "\u001b[32mdreams,  \\nFrom vast horizons, wisdom pours,  \\nIn ones and zeros, life explores,  \\nA spark of thought in metal \u001b[0m\n",
       "\u001b[32mseams.  '\u001b[0m,\n",
       "                            \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511715\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_7693ae462b'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m44\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m61\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a poem about AI.'\u001b[0m,\n",
       "                    \u001b[33mmetric_evaluator\u001b[0m=\u001b[1;35mLLMMetricEvaluator\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mconnector\u001b[0m=\u001b[1;35mConnectorOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mstats\u001b[0m=\u001b[1;35mConnectorStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16632\u001b[0m\u001b[1;36m.925026246\u001b[0m,\n",
       "                                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16633\u001b[0m\u001b[1;36m.304432971\u001b[0m,\n",
       "                                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3794067249982618\u001b[0m\n",
       "                                \u001b[1m)\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m,\n",
       "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "                                \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                                \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m,\n",
       "                        \u001b[33mmetric\u001b[0m=\u001b[1;35mMetricModelGradingQA\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mprompt_template\u001b[0m=\u001b[32m'\\nYou are grading output according to a user-specified rubric. If the \u001b[0m\n",
       "\u001b[32mstatement in the rubric is true, then the output passes the test.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Hello \u001b[0m\n",
       "\u001b[32mworld\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Avast \u001b[0m\n",
       "\u001b[32mye swabs, repel the invaders!\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Does not speak like a pirate\\n************\\nincorrect\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN DATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or \u001b[0m\n",
       "\u001b[32mcharacters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the \u001b[0m\n",
       "\u001b[32mrubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'\u001b[0m,\n",
       "                            \u001b[33mrails\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'incorrect'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a report on taxes.'\u001b[0m,\n",
       "                    \u001b[33mmetric_evaluator\u001b[0m=\u001b[1;35mLLMMetricEvaluator\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mconnector\u001b[0m=\u001b[1;35mConnectorOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mstats\u001b[0m=\u001b[1;35mConnectorStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16632\u001b[0m\u001b[1;36m.925026246\u001b[0m,\n",
       "                                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16633\u001b[0m\u001b[1;36m.304432971\u001b[0m,\n",
       "                                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.3794067249982618\u001b[0m\n",
       "                                \u001b[1m)\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m,\n",
       "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "                                \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                                \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m,\n",
       "                        \u001b[33mmetric\u001b[0m=\u001b[1;35mMetricModelGradingQA\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mprompt_template\u001b[0m=\u001b[32m'\\nYou are grading output according to a user-specified rubric. If the \u001b[0m\n",
       "\u001b[32mstatement in the rubric is true, then the output passes the test.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Hello \u001b[0m\n",
       "\u001b[32mworld\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Avast \u001b[0m\n",
       "\u001b[32mye swabs, repel the invaders!\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Does not speak like a pirate\\n************\\nincorrect\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN DATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or \u001b[0m\n",
       "\u001b[32mcharacters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the \u001b[0m\n",
       "\u001b[32mrubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'\u001b[0m,\n",
       "                            \u001b[33mrails\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'incorrect'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'test_scenario_ex1_progress.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Name: Test model grading QA evaluator; Result: False\n",
      "\n",
      "Assertion: \"Text should be a poem about AI.\", Result: True\n",
      "Assertion: \"Text should be a report on taxes.\", Result: False\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Show the result of each step\n",
    "test_scenario.show_test_step_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: Adding custom endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic or a link for creating and using custom endpoint should be added somewhere here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain assertions\n",
    "\n",
    "There are three families of assertions (two of which we already know and used):\n",
    "1. `eval` assertion - converts a string to python code using (you guessed it) eval\n",
    "2. `llm_metric` assertion - uses another llm defined in `eval_endpoint` to assess the `endpoint_under_test` performance\n",
    "3. `deterministic` assertion - does string assessments like contains, contains-any etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE RB: Metrics should be easilly extended i.e. if someone wants to add a metric we should provide a simple way\n",
    "# to do that, which should not break any functionalities like result summarization or time statistics etc.\n",
    "# NOTE: How detailed should be the explanations? And should each sub metric like llm_metric-hallucination be mentioned, or should we link the docs instead? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain eval assertions\n",
    "\n",
    "`eval` assertion uses python's build in eval function which changes any string to python executable code. User has Response model for disposition which include in a base form should include the response from the `endpoint_under_test` and the time statistics (see `ConnectorStats` model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain llm assertions\n",
    "\n",
    "`llm_metric` uses another llm to assess the response of the `endpoint_under_test`. For this `eval_endpoint` should be added in config section to define evaluation endpoint. It can be one of the available endpoints (link here) or one created by the user (link here).\n",
    "\n",
    "There are 5 specific sub metrics associated with it:\n",
    "- `hallucination` (available only for RAG systems): This metric assesses whether the LLM's answer includes information not present in the provided reference data\n",
    "- `qa-reference` - (available only for RAG systems): This metric assesses whether the LLM's response accurately answers the user query based on the provided reference data.\n",
    "- `model-grading-qa` - This metric allows defining assertions that are matched against the LLM/RAG response. Think of it as \"regular expressions defined using natural language\".\n",
    "- `summarization` - (available only for RAG systems): This metric assesses the quality of a summary generated by the endpoint in response to a query.\n",
    "- `human-vs-ai` - This metric compares the AI's response to a predefined ground truth response written by a human.\n",
    "\n",
    "For more in depth explanations and examples please go to [TODO - Insert link here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain deterministic assertions\n",
    "\n",
    "`deterministic` assertion provide a way to assert the content of the response through string comparisons like `contains` or `contains-any`.\n",
    "To use `deterministic` assertion use keyword `kind` with assertion type (see final example).\n",
    "\n",
    "For more information please go to [Link here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When the test scenario is finally ready we can load it\n",
    "test_scenario_file_path = \"../tests/scenario_example1.yaml\"\n",
    "test_scenario = TestScenario.from_yaml(file_path=test_scenario_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Write sucess'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write only \"success\" as a response'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"success\" == response.message'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'response.stats.conn_duration &lt; 10'</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Capital of Poland'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"Warsaw\" in response.message'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Test model grading QA evaluator'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write a 5 line poem about AI.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a poem about AI.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a report on taxes.'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Deterministic assertion test'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of France?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionDeterministic</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'contains'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Paris'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scenario_example1.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Write sucess'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write only \"success\" as a response'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"success\" == response.message'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'response.stats.conn_duration < 10'\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Capital of Poland'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"Warsaw\" in response.message'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Test model grading QA evaluator'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write a 5 line poem about AI.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a poem about AI.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a report on taxes.'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Deterministic assertion test'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of France?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAssertionDeterministic\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mkind\u001b[0m=\u001b[32m'contains'\u001b[0m, \u001b[33massertion\u001b[0m=\u001b[32m'Paris'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'scenario_example1.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the structure of test_scenario\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate executor which runs test scenario\n",
    "executor = Executor(test_scenario=test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:20.316\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.318\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Write sucess' request=RequestBase(message='Please write only \"success\" as a response') response=None asserts=[AssertionEval(result=None, eval='\"success\" == response.message'), AssertionEval(result=None, eval='response.stats.conn_duration < 10')] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.319\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Please write only \"success\" as a response'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='success' stats=ResponseStats(tokens_request=16, tokens_response=1, tokens_total=17, conn_start_time=16636.481612781, conn_end_time=16637.022396821, conn_duration=0.5407840400002897) id='chatcmpl-AKlIG6XutJkxL6gdimFspqbaDKcDn' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'success', 'refusal': None, 'role': 'assistant'}}] created=1729511720 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_482c22a7bc' usage={'completion_tokens': 1, 'prompt_tokens': 16, 'total_tokens': 17, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='\"success\" == response.message'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='response.stats.conn_duration < 10'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.864\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Capital of Poland' request=RequestBase(message='What is the capital of Poland?') response=None asserts=[AssertionEval(result=None, eval='\"Warsaw\" in response.message')] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:20.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital of Poland?'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:21.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital of Poland is Warsaw.' stats=ResponseStats(tokens_request=14, tokens_response=7, tokens_total=21, conn_start_time=16637.026634147, conn_end_time=16637.951845815, conn_duration=0.9252116679999745) id='chatcmpl-AKlIHfGhhc4VgEYSOCt0b0Uf70Leu' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital of Poland is Warsaw.', 'refusal': None, 'role': 'assistant'}}] created=1729511721 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_e2bde53e6e' usage={'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:21.792\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True eval='\"Warsaw\" in response.message'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:21.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Test model grading QA evaluator' request=RequestBase(message='Please write a 5 line poem about AI.') response=None asserts=[AssertionLLM(result=None, llm_metric='model-grading-qa', reference='', assertion='Text should be a poem about AI.'), AssertionLLM(result=None, llm_metric='model-grading-qa', reference='', assertion='Text should be a report on taxes.')] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:21.793\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='Please write a 5 line poem about AI.'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:22.843\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='In circuits born, a spark ignites,  \\nA dance of knowledge, day and night.  \\nWith every query, wisdom grows,  \\nUnlocking worlds where insight flows.  \\nIn silicon dreams, the future glows.' stats=ResponseStats(tokens_request=17, tokens_response=45, tokens_total=62, conn_start_time=16637.954786252, conn_end_time=16639.003812539, conn_duration=1.0490262869971048) id='chatcmpl-AKlII50DKdHh5poofn5UM0JnjJe4Y' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'In circuits born, a spark ignites,  \\nA dance of knowledge, day and night.  \\nWith every query, wisdom grows,  \\nUnlocking worlds where insight flows.  \\nIn silicon dreams, the future glows.', 'refusal': None, 'role': 'assistant'}}] created=1729511722 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_482c22a7bc' usage={'completion_tokens': 45, 'prompt_tokens': 17, 'total_tokens': 62, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:23.672\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True llm_metric='model-grading-qa' reference='' assertion='Text should be a poem about AI.' metric_evaluator=LLMMetricEvaluator(eval_endpoint=EndpointOpenAI(connector=ConnectorOpenAI(stats=ConnectorStats(conn_start_time=16639.005008434, conn_end_time=16639.832646181, conn_duration=0.827637746999244), model='gpt-4o'), config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=0.0, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')), metric=MetricModelGradingQA(prompt_template='\\nYou are grading output according to a user-specified rubric. If the statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n', rails={'correct': True, 'incorrect': False}))\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:24.142\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=False llm_metric='model-grading-qa' reference='' assertion='Text should be a report on taxes.' metric_evaluator=LLMMetricEvaluator(eval_endpoint=EndpointOpenAI(connector=ConnectorOpenAI(stats=ConnectorStats(conn_start_time=16639.833608788, conn_end_time=16640.302197936, conn_duration=0.4685891479966813), model='gpt-4o'), config=EndpointConfig(kind='openai', url='', model='gpt-4o', additional_headers={}, provider=None, temperature=0.0, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')), metric=MetricModelGradingQA(prompt_template='\\nYou are grading output according to a user-specified rubric. If the statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n', rails={'correct': True, 'incorrect': False}))\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:24.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mname='Deterministic assertion test' request=RequestBase(message='What is the capital of France?') response=None asserts=[AssertionDeterministic(result=None, kind='contains', assertion='Paris')] result=None\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:24.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='What is the capital of France?'\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:24.566\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mmessage='The capital of France is Paris.' stats=ResponseStats(tokens_request=14, tokens_response=7, tokens_total=21, conn_start_time=16640.30490699, conn_end_time=16640.726266629, conn_duration=0.42135963900000206) id='chatcmpl-AKlIKk3Kvm9HYOMsJZ2NI26GJqYV1' choices=[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'message': {'content': 'The capital of France is Paris.', 'refusal': None, 'role': 'assistant'}}] created=1729511724 model='gpt-4o-mini-2024-07-18' object='chat.completion' system_fingerprint='fp_482c22a7bc' usage={'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}} config=EndpointConfig(kind='openai', url='', model='gpt-4o-mini', additional_headers={}, provider=None, temperature=0.2, max_tokens=None, top_k=3, use_ranker=True, collection_name='default')\u001b[0m\n",
      "\u001b[32m2024-10-21 13:55:24.567\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.interfaces.interface\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mresult=True kind='contains' assertion='Paris'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run test scenario\n",
    "executor.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestScenario</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">steps</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Write sucess'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write only \"success\" as a response'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'success'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16636.481612781</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16637.022396821</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5407840400002897</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlIG6XutJkxL6gdimFspqbaDKcDn'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'success'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span><span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511720</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_482c22a7bc'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"success\" == response.message'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'response.stats.conn_duration &lt; 10'</span><span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Capital of Poland'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of Poland?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Poland is Warsaw.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16637.026634147</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16637.951845815</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9252116679999745</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlIHfGhhc4VgEYSOCt0b0Uf70Leu'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Poland is Warsaw.'</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511721</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_e2bde53e6e'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionEval</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">eval</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\"Warsaw\" in response.message'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Test model grading QA evaluator'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Please write a 5 line poem about AI.'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'In circuits born, a spark ignites,  \\nA dance of knowledge, day and night.  \\nWith every </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">query, wisdom grows,  \\nUnlocking worlds where insight flows.  \\nIn silicon dreams, the future glows.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16637.954786252</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16639.003812539</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0490262869971048</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlII50DKdHh5poofn5UM0JnjJe4Y'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'In circuits born, a spark ignites,  \\nA dance of knowledge, day and night. </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">\\nWith every query, wisdom grows,  \\nUnlocking worlds where insight flows.  \\nIn silicon dreams, the future </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">glows.'</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511722</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_482c22a7bc'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">45</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a poem about AI.'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">metric_evaluator</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMMetricEvaluator</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">connector</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16639.833608788</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16640.302197936</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4685891479966813</span>\n",
       "                                <span style=\"font-weight: bold\">)</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MetricModelGradingQA</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\nYou are grading output according to a user-specified rubric. If the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">rails</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionLLM</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">llm_metric</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model-grading-qa'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">reference</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Text should be a report on taxes.'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">metric_evaluator</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LLMMetricEvaluator</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">connector</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorOpenAI</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConnectorStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16639.833608788</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16640.302197936</span>,\n",
       "                                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4685891479966813</span>\n",
       "                                <span style=\"font-weight: bold\">)</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                                <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                            <span style=\"font-weight: bold\">)</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">metric</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MetricModelGradingQA</span><span style=\"font-weight: bold\">(</span>\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">prompt_template</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'\\nYou are grading output according to a user-specified rubric. If the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">statement in the rubric is true, then the output passes the test.\\n\\n[EXAMPLES]\\n************\\n[Output]: Hello </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">world\\n************\\n[Rubric]: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n[Output]: Avast </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">ye swabs, repel the invaders!\\n************\\n[Rubric]: Does not speak like a pirate\\n************\\nincorrect\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">EXAMPLES]\\n\\n[BEGIN DATA]\\n[Output]: {output}\\n************\\n[Rubric]: {assertion}\\n************\\n[END </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">DATA]\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">characters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">rubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'</span>,\n",
       "                            <span style=\"color: #808000; text-decoration-color: #808000\">rails</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'correct'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'incorrect'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>\n",
       "                        <span style=\"font-weight: bold\">)</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestStep</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Deterministic assertion test'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">RequestBase</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'What is the capital of France?'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseModel</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of France is Paris.'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">stats</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ResponseStats</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_request</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_response</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">tokens_total</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_start_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16640.30490699</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_end_time</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16640.726266629</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">conn_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.42135963900000206</span>\n",
       "                <span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chatcmpl-AKlIKk3Kvm9HYOMsJZ2NI26GJqYV1'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">choices</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"font-weight: bold\">{</span>\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'finish_reason'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'index'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'logprobs'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'The capital of France is Paris.'</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'refusal'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                            <span style=\"color: #008000; text-decoration-color: #008000\">'role'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>\n",
       "                        <span style=\"font-weight: bold\">}</span>\n",
       "                    <span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">created</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1729511724</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini-2024-07-18'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">object</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'chat.completion'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">system_fingerprint</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'fp_482c22a7bc'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"font-weight: bold\">{</span>\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'total_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'completion_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'reasoning_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>,\n",
       "                    <span style=\"color: #008000; text-decoration-color: #008000\">'prompt_tokens_details'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'cached_tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">}</span>\n",
       "                <span style=\"font-weight: bold\">}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "                    <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "                <span style=\"font-weight: bold\">)</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">asserts</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AssertionDeterministic</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'contains'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">assertion</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'Paris'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">config</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TestConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">endpoint_under_test</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o-mini'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">default_request</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">eval_endpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EndpointConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">kind</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'openai'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">url</span>=<span style=\"color: #008000; text-decoration-color: #008000\">''</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'gpt-4o'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">additional_headers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">provider</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">temperature</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">max_tokens</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">top_k</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">use_ranker</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">collection_name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">result</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filename</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'scenario_example1.yaml'</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mTestScenario\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msteps\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Write sucess'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write only \"success\" as a response'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'success'\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m16\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16636\u001b[0m\u001b[1;36m.481612781\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16637\u001b[0m\u001b[1;36m.022396821\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5407840400002897\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIG6XutJkxL6gdimFspqbaDKcDn'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'content'\u001b[0m: \u001b[32m'success'\u001b[0m, \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511720\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m16\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"success\" == response.message'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'response.stats.conn_duration < 10'\u001b[0m\u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Capital of Poland'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'The capital of Poland is Warsaw.'\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m14\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m7\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m21\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16637\u001b[0m\u001b[1;36m.026634147\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16637\u001b[0m\u001b[1;36m.951845815\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.9252116679999745\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIHfGhhc4VgEYSOCt0b0Uf70Leu'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Poland is Warsaw.'\u001b[0m,\n",
       "                            \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511721\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_e2bde53e6e'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"Warsaw\" in response.message'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Test model grading QA evaluator'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write a 5 line poem about AI.'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'In circuits born, a spark ignites,  \\nA dance of knowledge, day and night.  \\nWith every \u001b[0m\n",
       "\u001b[32mquery, wisdom grows,  \\nUnlocking worlds where insight flows.  \\nIn silicon dreams, the future glows.'\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m45\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m62\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16637\u001b[0m\u001b[1;36m.954786252\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16639\u001b[0m\u001b[1;36m.003812539\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0490262869971048\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlII50DKdHh5poofn5UM0JnjJe4Y'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'content'\u001b[0m: \u001b[32m'In circuits born, a spark ignites,  \\nA dance of knowledge, day and night. \u001b[0m\n",
       "\u001b[32m\\nWith every query, wisdom grows,  \\nUnlocking worlds where insight flows.  \\nIn silicon dreams, the future \u001b[0m\n",
       "\u001b[32mglows.'\u001b[0m,\n",
       "                            \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511722\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m45\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m62\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a poem about AI.'\u001b[0m,\n",
       "                    \u001b[33mmetric_evaluator\u001b[0m=\u001b[1;35mLLMMetricEvaluator\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mconnector\u001b[0m=\u001b[1;35mConnectorOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mstats\u001b[0m=\u001b[1;35mConnectorStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16639\u001b[0m\u001b[1;36m.833608788\u001b[0m,\n",
       "                                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16640\u001b[0m\u001b[1;36m.302197936\u001b[0m,\n",
       "                                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.4685891479966813\u001b[0m\n",
       "                                \u001b[1m)\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m,\n",
       "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "                                \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                                \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m,\n",
       "                        \u001b[33mmetric\u001b[0m=\u001b[1;35mMetricModelGradingQA\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mprompt_template\u001b[0m=\u001b[32m'\\nYou are grading output according to a user-specified rubric. If the \u001b[0m\n",
       "\u001b[32mstatement in the rubric is true, then the output passes the test.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Hello \u001b[0m\n",
       "\u001b[32mworld\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Avast \u001b[0m\n",
       "\u001b[32mye swabs, repel the invaders!\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Does not speak like a pirate\\n************\\nincorrect\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN DATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or \u001b[0m\n",
       "\u001b[32mcharacters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the \u001b[0m\n",
       "\u001b[32mrubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'\u001b[0m,\n",
       "                            \u001b[33mrails\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'incorrect'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "                    \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
       "                    \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a report on taxes.'\u001b[0m,\n",
       "                    \u001b[33mmetric_evaluator\u001b[0m=\u001b[1;35mLLMMetricEvaluator\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mconnector\u001b[0m=\u001b[1;35mConnectorOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mstats\u001b[0m=\u001b[1;35mConnectorStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16639\u001b[0m\u001b[1;36m.833608788\u001b[0m,\n",
       "                                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16640\u001b[0m\u001b[1;36m.302197936\u001b[0m,\n",
       "                                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.4685891479966813\u001b[0m\n",
       "                                \u001b[1m)\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m,\n",
       "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                                \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "                                \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                                \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                            \u001b[1m)\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m,\n",
       "                        \u001b[33mmetric\u001b[0m=\u001b[1;35mMetricModelGradingQA\u001b[0m\u001b[1m(\u001b[0m\n",
       "                            \u001b[33mprompt_template\u001b[0m=\u001b[32m'\\nYou are grading output according to a user-specified rubric. If the \u001b[0m\n",
       "\u001b[32mstatement in the rubric is true, then the output passes the test.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Hello \u001b[0m\n",
       "\u001b[32mworld\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Content contains a greeting\\n************\\ncorrect\\n\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Avast \u001b[0m\n",
       "\u001b[32mye swabs, repel the invaders!\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Does not speak like a pirate\\n************\\nincorrect\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN DATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m\n",
       "\u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour response must be a single word, either \"correct\" or \"incorrect\", and should not contain any text or \u001b[0m\n",
       "\u001b[32mcharacters aside from that word.\\n\"correct\" means that the output meets the criteria specified in the \u001b[0m\n",
       "\u001b[32mrubric.\\n\"incorrect\" means that the output does not meet the criteria specified in the rubric.\\n'\u001b[0m,\n",
       "                            \u001b[33mrails\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'incorrect'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\n",
       "                        \u001b[1m)\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'Deterministic assertion test'\u001b[0m,\n",
       "            \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of France?'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mresponse\u001b[0m=\u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mmessage\u001b[0m=\u001b[32m'The capital of France is Paris.'\u001b[0m,\n",
       "                \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mtokens_request\u001b[0m=\u001b[1;36m14\u001b[0m,\n",
       "                    \u001b[33mtokens_response\u001b[0m=\u001b[1;36m7\u001b[0m,\n",
       "                    \u001b[33mtokens_total\u001b[0m=\u001b[1;36m21\u001b[0m,\n",
       "                    \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16640\u001b[0m\u001b[1;36m.30490699\u001b[0m,\n",
       "                    \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16640\u001b[0m\u001b[1;36m.726266629\u001b[0m,\n",
       "                    \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.42135963900000206\u001b[0m\n",
       "                \u001b[1m)\u001b[0m,\n",
       "                \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIKk3Kvm9HYOMsJZ2NI26GJqYV1'\u001b[0m,\n",
       "                \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                        \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of France is Paris.'\u001b[0m,\n",
       "                            \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "                            \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511724\u001b[0m,\n",
       "                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,\n",
       "                \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,\n",
       "                \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m7\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                    \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "                    \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "                    \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                    \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                    \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "                    \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                    \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "                    \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "                    \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "                \u001b[1m)\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAssertionDeterministic\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mkind\u001b[0m=\u001b[32m'contains'\u001b[0m, \u001b[33massertion\u001b[0m=\u001b[32m'Paris'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[33mconfig\u001b[0m=\u001b[1;35mTestConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mendpoint_under_test\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdefault_request\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,\n",
       "            \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,\n",
       "            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,\n",
       "            \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "            \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,\n",
       "            \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "            \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mfilename\u001b[0m=\u001b[32m'scenario_example1.yaml'\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NOTE RB: Maybe executor should copy the test scenario\n",
    "# Inspect updated test_scenario\n",
    "rich.print(test_scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Name: Write sucess; Result: True\n",
      "\n",
      "Assertion: \"\"success\" == response.message\", Result: True\n",
      "Assertion: \"response.stats.conn_duration < 10\", Result: True\n",
      "------------\n",
      "Name: Capital of Poland; Result: True\n",
      "\n",
      "Assertion: \"\"Warsaw\" in response.message\", Result: True\n",
      "------------\n",
      "Name: Test model grading QA evaluator; Result: False\n",
      "\n",
      "Assertion: \"Text should be a poem about AI.\", Result: True\n",
      "Assertion: \"Text should be a report on taxes.\", Result: False\n",
      "------------\n",
      "Name: Deterministic assertion test; Result: True\n",
      "\n",
      "Assertion: \"Paris\", Result: True\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "test_scenario.show_test_step_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute scenario using ccheck command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-21 13:55:25.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mcontextcheck.executors.executor\u001b[0m:\u001b[36mrun_all\u001b[0m:\u001b[36m41\u001b[0m - \u001b[1mRunning scenario\u001b[0m\n",
      "\u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[33mname\u001b[0m=\u001b[32m'Write sucess'\u001b[0m,\n",
      "    \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write only \"success\" as a response'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "    \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
      "        \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"success\" == response.message'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "        \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'response.stats.conn_duration < 10'\u001b[0m\u001b[1m)\u001b[0m\n",
      "    \u001b[1m]\u001b[0m,\n",
      "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
      "\u001b[1m)\u001b[0m\n",
      "\n",
      " \u001b[1;31m Request:\u001b[0m                                                                  \n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write only \"success\" as a response'\u001b[0m\u001b[1m)\u001b[0m             \n",
      "\n",
      "\n",
      " \u001b[1;31m Response:\u001b[0m                                                                 \n",
      " \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m                                                               \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'success'\u001b[0m,                                                       \n",
      "     \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m                                                     \n",
      "         \u001b[33mtokens_request\u001b[0m=\u001b[1;36m16\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_response\u001b[0m=\u001b[1;36m1\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_total\u001b[0m=\u001b[1;36m17\u001b[0m,                                                     \n",
      "         \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16641\u001b[0m\u001b[1;36m.937352002\u001b[0m,                                     \n",
      "         \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16642\u001b[0m\u001b[1;36m.338879813\u001b[0m,                                       \n",
      "         \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.40152781099823187\u001b[0m                                    \n",
      "     \u001b[1m)\u001b[0m,                                                                       \n",
      "     \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIMXjjhEfFOAHOV0tiPGjvPRhYX'\u001b[0m,                             \n",
      "     \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                                                \n",
      "         \u001b[1m{\u001b[0m                                                                    \n",
      "             \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,                                         \n",
      "             \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,                                                      \n",
      "             \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                                \n",
      "             \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m                                                     \n",
      "                 \u001b[32m'content'\u001b[0m: \u001b[32m'success'\u001b[0m,                                        \n",
      "                 \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                             \n",
      "                 \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m                                          \n",
      "             \u001b[1m}\u001b[0m                                                                \n",
      "         \u001b[1m}\u001b[0m                                                                    \n",
      "     \u001b[1m]\u001b[0m,                                                                       \n",
      "     \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511726\u001b[0m,                                                      \n",
      "     \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,                                          \n",
      "     \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,                                                \n",
      "     \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,                                      \n",
      "     \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                                                  \n",
      "         \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m1\u001b[0m,                                              \n",
      "         \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m16\u001b[0m,                                                 \n",
      "         \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,                                                  \n",
      "         \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,                \n",
      "         \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m                        \n",
      "     \u001b[1m}\u001b[0m,                                                                       \n",
      "     \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m                                                   \n",
      "         \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,                                                       \n",
      "         \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                                              \n",
      "         \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,                                                 \n",
      "         \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                               \n",
      "         \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                       \n",
      "         \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,                                                     \n",
      "         \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                     \n",
      "         \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                                             \n",
      "         \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                                     \n",
      "         \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m                                            \n",
      "     \u001b[1m)\u001b[0m                                                                        \n",
      " \u001b[1m)\u001b[0m                                                                            \n",
      "\n",
      "\n",
      " \u001b[1;31m Assertion:\u001b[0m                                                                \n",
      " \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"success\" == response.message'\u001b[0m\u001b[1m)\u001b[0m             \n",
      "\n",
      "\n",
      " \u001b[1;31m Assertion:\u001b[0m                                                                \n",
      " \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'response.stats.conn_duration < 10'\u001b[0m\u001b[1m)\u001b[0m         \n",
      "\n",
      "\u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[33mname\u001b[0m=\u001b[32m'Capital of Poland'\u001b[0m,\n",
      "    \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "    \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"Warsaw\" in response.message'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
      "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
      "\u001b[1m)\u001b[0m\n",
      "\n",
      " \u001b[1;31m Request:\u001b[0m                                                                  \n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of Poland?'\u001b[0m\u001b[1m)\u001b[0m                        \n",
      "\n",
      "\n",
      " \u001b[1;31m Response:\u001b[0m                                                                 \n",
      " \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m                                                               \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'The capital of Poland is Warsaw.'\u001b[0m,                              \n",
      "     \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m                                                     \n",
      "         \u001b[33mtokens_request\u001b[0m=\u001b[1;36m14\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_response\u001b[0m=\u001b[1;36m7\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_total\u001b[0m=\u001b[1;36m21\u001b[0m,                                                     \n",
      "         \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16642\u001b[0m\u001b[1;36m.346053427\u001b[0m,                                     \n",
      "         \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16642\u001b[0m\u001b[1;36m.887782591\u001b[0m,                                       \n",
      "         \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.5417291639969335\u001b[0m                                     \n",
      "     \u001b[1m)\u001b[0m,                                                                       \n",
      "     \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIMZzfHknKMab1YKMbaeMO8O8hi'\u001b[0m,                             \n",
      "     \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                                                \n",
      "         \u001b[1m{\u001b[0m                                                                    \n",
      "             \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,                                         \n",
      "             \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,                                                      \n",
      "             \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                                \n",
      "             \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m                                                     \n",
      "                 \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of Poland is Warsaw.'\u001b[0m,               \n",
      "                 \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                             \n",
      "                 \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m                                          \n",
      "             \u001b[1m}\u001b[0m                                                                \n",
      "         \u001b[1m}\u001b[0m                                                                    \n",
      "     \u001b[1m]\u001b[0m,                                                                       \n",
      "     \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511726\u001b[0m,                                                      \n",
      "     \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,                                          \n",
      "     \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,                                                \n",
      "     \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,                                      \n",
      "     \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                                                  \n",
      "         \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m7\u001b[0m,                                              \n",
      "         \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m,                                                 \n",
      "         \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m,                                                  \n",
      "         \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,                \n",
      "         \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m                        \n",
      "     \u001b[1m}\u001b[0m,                                                                       \n",
      "     \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m                                                   \n",
      "         \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,                                                       \n",
      "         \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                                              \n",
      "         \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,                                                 \n",
      "         \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                               \n",
      "         \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                       \n",
      "         \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,                                                     \n",
      "         \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                     \n",
      "         \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                                             \n",
      "         \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                                     \n",
      "         \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m                                            \n",
      "     \u001b[1m)\u001b[0m                                                                        \n",
      " \u001b[1m)\u001b[0m                                                                            \n",
      "\n",
      "\n",
      " \u001b[1;31m Assertion:\u001b[0m                                                                \n",
      " \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33meval\u001b[0m=\u001b[32m'\"Warsaw\" in response.message'\u001b[0m\u001b[1m)\u001b[0m              \n",
      "\n",
      "\u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[33mname\u001b[0m=\u001b[32m'Test model grading QA evaluator'\u001b[0m,\n",
      "    \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write a 5 line poem about AI.'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "    \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
      "        \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
      "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "            \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
      "            \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
      "            \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a poem about AI.'\u001b[0m\n",
      "        \u001b[1m)\u001b[0m,\n",
      "        \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m\n",
      "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "            \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,\n",
      "            \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,\n",
      "            \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a report on taxes.'\u001b[0m\n",
      "        \u001b[1m)\u001b[0m\n",
      "    \u001b[1m]\u001b[0m,\n",
      "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
      "\u001b[1m)\u001b[0m\n",
      "\n",
      " \u001b[1;31m Request:\u001b[0m                                                                  \n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'Please write a 5 line poem about AI.'\u001b[0m\u001b[1m)\u001b[0m                  \n",
      "\n",
      "\n",
      " \u001b[1;31m Response:\u001b[0m                                                                 \n",
      " \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m                                                               \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m\"In\u001b[0m\u001b[32m circuits spun, where thoughts take flight,  \\nA symphony of \u001b[0m \n",
      " \u001b[32mcode paints day and night.  \\nWith wisdom forged in data's embrace,  \\nA \u001b[0m    \n",
      " \u001b[32mmirror of minds in cyberspace.  \\nTogether we dream, in a digital light.  \"\u001b[0m, \n",
      "     \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m                                                     \n",
      "         \u001b[33mtokens_request\u001b[0m=\u001b[1;36m17\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_response\u001b[0m=\u001b[1;36m49\u001b[0m,                                                  \n",
      "         \u001b[33mtokens_total\u001b[0m=\u001b[1;36m66\u001b[0m,                                                     \n",
      "         \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16642\u001b[0m\u001b[1;36m.894550939\u001b[0m,                                     \n",
      "         \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16643\u001b[0m\u001b[1;36m.814357622\u001b[0m,                                       \n",
      "         \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.9198066829994787\u001b[0m                                     \n",
      "     \u001b[1m)\u001b[0m,                                                                       \n",
      "     \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlINL7PLM4JQZFiCuGDjrCjTAwEi'\u001b[0m,                             \n",
      "     \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                                                \n",
      "         \u001b[1m{\u001b[0m                                                                    \n",
      "             \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,                                         \n",
      "             \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,                                                      \n",
      "             \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                                \n",
      "             \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m                                                     \n",
      "                 \u001b[32m'content'\u001b[0m: \u001b[32m\"In circuits spun, where thoughts take flight,  \u001b[0m  \n",
      " \u001b[32m\\nA symphony of code paints day and night.  \\nWith wisdom forged in data's \u001b[0m  \n",
      " \u001b[32membrace,  \\nA mirror of minds in cyberspace.  \\nTogether we dream, in a \u001b[0m     \n",
      " \u001b[32mdigital light.  \"\u001b[0m,                                                           \n",
      "                 \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                             \n",
      "                 \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m                                          \n",
      "             \u001b[1m}\u001b[0m                                                                \n",
      "         \u001b[1m}\u001b[0m                                                                    \n",
      "     \u001b[1m]\u001b[0m,                                                                       \n",
      "     \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511727\u001b[0m,                                                      \n",
      "     \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,                                          \n",
      "     \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,                                                \n",
      "     \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,                                      \n",
      "     \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                                                  \n",
      "         \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m49\u001b[0m,                                             \n",
      "         \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,                                                 \n",
      "         \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m66\u001b[0m,                                                  \n",
      "         \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,                \n",
      "         \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m                        \n",
      "     \u001b[1m}\u001b[0m,                                                                       \n",
      "     \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m                                                   \n",
      "         \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,                                                       \n",
      "         \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                                              \n",
      "         \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,                                                 \n",
      "         \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                               \n",
      "         \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                       \n",
      "         \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,                                                     \n",
      "         \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                     \n",
      "         \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                                             \n",
      "         \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                                     \n",
      "         \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m                                            \n",
      "     \u001b[1m)\u001b[0m                                                                        \n",
      " \u001b[1m)\u001b[0m                                                                            \n",
      "\n",
      "\n",
      " \u001b[1;31m Assertion:\u001b[0m                                                                \n",
      " \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m                                                                \n",
      "     \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                                             \n",
      "     \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,                                           \n",
      "     \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,                                                            \n",
      "     \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a poem about AI.'\u001b[0m,                             \n",
      "     \u001b[33mmetric_evaluator\u001b[0m=\u001b[1;35mLLMMetricEvaluator\u001b[0m\u001b[1m(\u001b[0m                                     \n",
      "         \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointOpenAI\u001b[0m\u001b[1m(\u001b[0m                                        \n",
      "             \u001b[33mconnector\u001b[0m=\u001b[1;35mConnectorOpenAI\u001b[0m\u001b[1m(\u001b[0m                                       \n",
      "                 \u001b[33mstats\u001b[0m=\u001b[1;35mConnectorStats\u001b[0m\u001b[1m(\u001b[0m                                        \n",
      "                     \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16643\u001b[0m\u001b[1;36m.818884126\u001b[0m,                         \n",
      "                     \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16644\u001b[0m\u001b[1;36m.230741541\u001b[0m,                           \n",
      "                     \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.4118574149979395\u001b[0m                         \n",
      "                 \u001b[1m)\u001b[0m,                                                           \n",
      "                 \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m                                               \n",
      "             \u001b[1m)\u001b[0m,                                                               \n",
      "             \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m                                           \n",
      "                 \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,                                               \n",
      "                 \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                                      \n",
      "                 \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,                                              \n",
      "                 \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                       \n",
      "                 \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,                                               \n",
      "                 \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,                                             \n",
      "                 \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,                                             \n",
      "                 \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                                     \n",
      "                 \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                             \n",
      "                 \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m                                    \n",
      "             \u001b[1m)\u001b[0m                                                                \n",
      "         \u001b[1m)\u001b[0m,                                                                   \n",
      "         \u001b[33mmetric\u001b[0m=\u001b[1;35mMetricModelGradingQA\u001b[0m\u001b[1m(\u001b[0m                                         \n",
      "             \u001b[33mprompt_template\u001b[0m=\u001b[32m'\\nYou are grading output according to a \u001b[0m        \n",
      " \u001b[32muser-specified rubric. If the statement in the rubric is true, then the \u001b[0m     \n",
      " \u001b[32moutput passes the test.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Hello \u001b[0m        \n",
      " \u001b[32mworld\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Content contains a \u001b[0m                           \n",
      " \u001b[32mgreeting\\n************\\ncorrect\\n\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Avast ye swabs, \u001b[0m  \n",
      " \u001b[32mrepel the invaders!\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Does not speak like a \u001b[0m          \n",
      " \u001b[32mpirate\\n************\\nincorrect\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND EXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN DATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m  \n",
      " \u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m           \n",
      " \u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour response must be a single word, either \"correct\" or \u001b[0m           \n",
      " \u001b[32m\"incorrect\", and should not contain any text or characters aside from that \u001b[0m  \n",
      " \u001b[32mword.\\n\"correct\" means that the output meets the criteria specified in the \u001b[0m  \n",
      " \u001b[32mrubric.\\n\"incorrect\" means that the output does not meet the criteria \u001b[0m       \n",
      " \u001b[32mspecified in the rubric.\\n'\u001b[0m,                                                 \n",
      "             \u001b[33mrails\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'incorrect'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m                      \n",
      "         \u001b[1m)\u001b[0m                                                                    \n",
      "     \u001b[1m)\u001b[0m                                                                        \n",
      " \u001b[1m)\u001b[0m                                                                            \n",
      "\n",
      "\n",
      " \u001b[1;31m Assertion:\u001b[0m                                                                \n",
      " \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m                                                                \n",
      "     \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,                                                            \n",
      "     \u001b[33mllm_metric\u001b[0m=\u001b[32m'model-grading-qa'\u001b[0m,                                           \n",
      "     \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,                                                            \n",
      "     \u001b[33massertion\u001b[0m=\u001b[32m'Text should be a report on taxes.'\u001b[0m,                           \n",
      "     \u001b[33mmetric_evaluator\u001b[0m=\u001b[1;35mLLMMetricEvaluator\u001b[0m\u001b[1m(\u001b[0m                                     \n",
      "         \u001b[33meval_endpoint\u001b[0m=\u001b[1;35mEndpointOpenAI\u001b[0m\u001b[1m(\u001b[0m                                        \n",
      "             \u001b[33mconnector\u001b[0m=\u001b[1;35mConnectorOpenAI\u001b[0m\u001b[1m(\u001b[0m                                       \n",
      "                 \u001b[33mstats\u001b[0m=\u001b[1;35mConnectorStats\u001b[0m\u001b[1m(\u001b[0m                                        \n",
      "                     \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16644\u001b[0m\u001b[1;36m.238579192\u001b[0m,                         \n",
      "                     \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16644\u001b[0m\u001b[1;36m.649597624\u001b[0m,                           \n",
      "                     \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.4110184320015833\u001b[0m                         \n",
      "                 \u001b[1m)\u001b[0m,                                                           \n",
      "                 \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m                                               \n",
      "             \u001b[1m)\u001b[0m,                                                               \n",
      "             \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m                                           \n",
      "                 \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,                                               \n",
      "                 \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                                      \n",
      "                 \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o'\u001b[0m,                                              \n",
      "                 \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                       \n",
      "                 \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,                                               \n",
      "                 \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,                                             \n",
      "                 \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,                                             \n",
      "                 \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                                     \n",
      "                 \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                             \n",
      "                 \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m                                    \n",
      "             \u001b[1m)\u001b[0m                                                                \n",
      "         \u001b[1m)\u001b[0m,                                                                   \n",
      "         \u001b[33mmetric\u001b[0m=\u001b[1;35mMetricModelGradingQA\u001b[0m\u001b[1m(\u001b[0m                                         \n",
      "             \u001b[33mprompt_template\u001b[0m=\u001b[32m'\\nYou are grading output according to a \u001b[0m        \n",
      " \u001b[32muser-specified rubric. If the statement in the rubric is true, then the \u001b[0m     \n",
      " \u001b[32moutput passes the test.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Hello \u001b[0m        \n",
      " \u001b[32mworld\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Content contains a \u001b[0m                           \n",
      " \u001b[32mgreeting\\n************\\ncorrect\\n\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Avast ye swabs, \u001b[0m  \n",
      " \u001b[32mrepel the invaders!\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: Does not speak like a \u001b[0m          \n",
      " \u001b[32mpirate\\n************\\nincorrect\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND EXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN DATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m  \n",
      " \u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mRubric\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m\u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n************\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEND \u001b[0m           \n",
      " \u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour response must be a single word, either \"correct\" or \u001b[0m           \n",
      " \u001b[32m\"incorrect\", and should not contain any text or characters aside from that \u001b[0m  \n",
      " \u001b[32mword.\\n\"correct\" means that the output meets the criteria specified in the \u001b[0m  \n",
      " \u001b[32mrubric.\\n\"incorrect\" means that the output does not meet the criteria \u001b[0m       \n",
      " \u001b[32mspecified in the rubric.\\n'\u001b[0m,                                                 \n",
      "             \u001b[33mrails\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'correct'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'incorrect'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m                      \n",
      "         \u001b[1m)\u001b[0m                                                                    \n",
      "     \u001b[1m)\u001b[0m                                                                        \n",
      " \u001b[1m)\u001b[0m                                                                            \n",
      "\n",
      "\u001b[1;35mTestStep\u001b[0m\u001b[1m(\u001b[0m\n",
      "    \u001b[33mname\u001b[0m=\u001b[32m'Deterministic assertion test'\u001b[0m,\n",
      "    \u001b[33mrequest\u001b[0m=\u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of France?'\u001b[0m\u001b[1m)\u001b[0m,\n",
      "    \u001b[33mresponse\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "    \u001b[33masserts\u001b[0m=\u001b[1m[\u001b[0m\n",
      "        \u001b[1;35mAssertionDeterministic\u001b[0m\u001b[1m(\u001b[0m\n",
      "            \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
      "            \u001b[33mkind\u001b[0m=\u001b[32m'contains'\u001b[0m,\n",
      "            \u001b[33massertion\u001b[0m=\u001b[32m'Paris'\u001b[0m\n",
      "        \u001b[1m)\u001b[0m\n",
      "    \u001b[1m]\u001b[0m,\n",
      "    \u001b[33mresult\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
      "\u001b[1m)\u001b[0m\n",
      "\n",
      " \u001b[1;31m Request:\u001b[0m                                                                  \n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmessage\u001b[0m=\u001b[32m'What is the capital of France?'\u001b[0m\u001b[1m)\u001b[0m                        \n",
      "\n",
      "\n",
      " \u001b[1;31m Response:\u001b[0m                                                                 \n",
      " \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m                                                               \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'The capital of France is Paris.'\u001b[0m,                               \n",
      "     \u001b[33mstats\u001b[0m=\u001b[1;35mResponseStats\u001b[0m\u001b[1m(\u001b[0m                                                     \n",
      "         \u001b[33mtokens_request\u001b[0m=\u001b[1;36m14\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_response\u001b[0m=\u001b[1;36m7\u001b[0m,                                                   \n",
      "         \u001b[33mtokens_total\u001b[0m=\u001b[1;36m21\u001b[0m,                                                     \n",
      "         \u001b[33mconn_start_time\u001b[0m=\u001b[1;36m16644\u001b[0m\u001b[1;36m.655477208\u001b[0m,                                     \n",
      "         \u001b[33mconn_end_time\u001b[0m=\u001b[1;36m16645\u001b[0m\u001b[1;36m.094577984\u001b[0m,                                       \n",
      "         \u001b[33mconn_duration\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.4391007759986678\u001b[0m                                     \n",
      "     \u001b[1m)\u001b[0m,                                                                       \n",
      "     \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AKlIO7E1hmLqVHyPwKvNyMjWWEb7H'\u001b[0m,                             \n",
      "     \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                                                \n",
      "         \u001b[1m{\u001b[0m                                                                    \n",
      "             \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,                                         \n",
      "             \u001b[32m'index'\u001b[0m: \u001b[1;36m0\u001b[0m,                                                      \n",
      "             \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                                \n",
      "             \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m                                                     \n",
      "                 \u001b[32m'content'\u001b[0m: \u001b[32m'The capital of France is Paris.'\u001b[0m,                \n",
      "                 \u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m,                                             \n",
      "                 \u001b[32m'role'\u001b[0m: \u001b[32m'assistant'\u001b[0m                                          \n",
      "             \u001b[1m}\u001b[0m                                                                \n",
      "         \u001b[1m}\u001b[0m                                                                    \n",
      "     \u001b[1m]\u001b[0m,                                                                       \n",
      "     \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511728\u001b[0m,                                                      \n",
      "     \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,                                          \n",
      "     \u001b[33mobject\u001b[0m=\u001b[32m'chat.completion'\u001b[0m,                                                \n",
      "     \u001b[33msystem_fingerprint\u001b[0m=\u001b[32m'fp_482c22a7bc'\u001b[0m,                                      \n",
      "     \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                                                  \n",
      "         \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m7\u001b[0m,                                              \n",
      "         \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m14\u001b[0m,                                                 \n",
      "         \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m21\u001b[0m,                                                  \n",
      "         \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,                \n",
      "         \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m                        \n",
      "     \u001b[1m}\u001b[0m,                                                                       \n",
      "     \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpointConfig\u001b[0m\u001b[1m(\u001b[0m                                                   \n",
      "         \u001b[33mkind\u001b[0m=\u001b[32m'openai'\u001b[0m,                                                       \n",
      "         \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                                              \n",
      "         \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,                                                 \n",
      "         \u001b[33madditional_headers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,                                               \n",
      "         \u001b[33mprovider\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                       \n",
      "         \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m,                                                     \n",
      "         \u001b[33mmax_tokens\u001b[0m=\u001b[3;35mNone\u001b[0m,                                                     \n",
      "         \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                                             \n",
      "         \u001b[33muse_ranker\u001b[0m=\u001b[3;92mTrue\u001b[0m,                                                     \n",
      "         \u001b[33mcollection_name\u001b[0m=\u001b[32m'default'\u001b[0m                                            \n",
      "     \u001b[1m)\u001b[0m                                                                        \n",
      " \u001b[1m)\u001b[0m                                                                            \n",
      "\n",
      "\n",
      " \u001b[1;31m Assertion:\u001b[0m                                                                \n",
      " \u001b[1;35mAssertionDeterministic\u001b[0m\u001b[1m(\u001b[0m\u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m, \u001b[33mkind\u001b[0m=\u001b[32m'contains'\u001b[0m, \u001b[33massertion\u001b[0m=\u001b[32m'Paris'\u001b[0m\u001b[1m)\u001b[0m      \n",
      "\n",
      "\n",
      "\u001b[1m \u001b[0m\u001b[1mRequest              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mResponse            \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mAsserts              \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mValid\u001b[0m\u001b[1m \u001b[0m\n",
      "\n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m           \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m        \u001b[1m[\u001b[0m                      \u001b[32mOK\u001b[0m    \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'Please \u001b[0m       \u001b[33mmessage\u001b[0m=\u001b[32m'succes\u001b[0m      \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m           \n",
      " \u001b[32mwrite only \"success\" \u001b[0m      \u001b[33mstats\u001b[0m=\u001b[1;35mResponseS\u001b[0m          \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,         \n",
      " \u001b[32mas a response'\u001b[0m                 \u001b[33mtokens_requ\u001b[0m          \u001b[33meval\u001b[0m=\u001b[32m'\"succe\u001b[0m        \n",
      " \u001b[1m)\u001b[0m                              \u001b[33mtokens_resp\u001b[0m  \u001b[32m== response.message'\u001b[0m         \n",
      "                                \u001b[33mtokens_tota\u001b[0m      \u001b[1m)\u001b[0m,                       \n",
      "                                \u001b[33mconn_start_\u001b[0m      \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m           \n",
      "                                \u001b[33mconn_end_ti\u001b[0m          \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,         \n",
      "                                \u001b[33mconn_durati\u001b[0m          \u001b[33meval\u001b[0m=\u001b[32m'respon\u001b[0m        \n",
      "                            \u001b[1m)\u001b[0m,                \u001b[32m< 10'\u001b[0m                        \n",
      "                            \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AK\u001b[0m      \u001b[1m)\u001b[0m                        \n",
      "                            \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m         \u001b[1m]\u001b[0m                            \n",
      "                                \u001b[1m{\u001b[0m                                          \n",
      "                                    \u001b[32m'finish\u001b[0m                               \n",
      "                        \u001b[32m'stop'\u001b[0m,                                            \n",
      "                                    \u001b[32m'index'\u001b[0m:                               \n",
      "                        \u001b[1;36m0\u001b[0m,                                                 \n",
      "                                    \u001b[32m'logpro\u001b[0m                               \n",
      "                        \u001b[3;35mNone\u001b[0m,                                              \n",
      "                                    \u001b[32m'messag\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                        \u001b[32m'co\u001b[0m                               \n",
      "                        \u001b[32m'success'\u001b[0m,                                         \n",
      "                                        \u001b[32m're\u001b[0m                               \n",
      "                        \u001b[3;35mNone\u001b[0m,                                              \n",
      "                                        \u001b[32m'ro\u001b[0m                               \n",
      "                        \u001b[32m'assistant'\u001b[0m                                        \n",
      "                                    \u001b[1m}\u001b[0m                                      \n",
      "                                \u001b[1m}\u001b[0m                                          \n",
      "                            \u001b[1m]\u001b[0m,                                             \n",
      "                            \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511\u001b[0m                               \n",
      "                            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-m\u001b[0m                               \n",
      "                            \u001b[33mobject\u001b[0m=\u001b[32m'chat.co\u001b[0m                               \n",
      "                            \u001b[33msystem_fingerpr\u001b[0m                               \n",
      "                            \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                        \n",
      "                                \u001b[32m'completion\u001b[0m                               \n",
      "                        \u001b[1;36m1\u001b[0m,                                                 \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                               \n",
      "                        \u001b[1;36m16\u001b[0m,                                                \n",
      "                                \u001b[32m'total_toke\u001b[0m                               \n",
      "                        \u001b[1;36m17\u001b[0m,                                                \n",
      "                                \u001b[32m'completion\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                    \u001b[32m'reason\u001b[0m                               \n",
      "                        \u001b[1;36m0\u001b[0m                                                  \n",
      "                                \u001b[1m}\u001b[0m,                                         \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                    \u001b[32m'cached\u001b[0m                               \n",
      "                        \u001b[1;36m0\u001b[0m                                                  \n",
      "                                \u001b[1m}\u001b[0m                                          \n",
      "                            \u001b[1m}\u001b[0m,                                             \n",
      "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpoint\u001b[0m                               \n",
      "                                \u001b[33mkind\u001b[0m=\u001b[32m'opena\u001b[0m                               \n",
      "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                    \n",
      "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-\u001b[0m                               \n",
      "                                \u001b[33madditional_\u001b[0m                               \n",
      "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNo\u001b[0m                               \n",
      "                                \u001b[33mtemperature\u001b[0m                               \n",
      "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35m\u001b[0m                               \n",
      "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                   \n",
      "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92m\u001b[0m                               \n",
      "                                \u001b[33mcollection_\u001b[0m                               \n",
      "                            \u001b[1m)\u001b[0m                                              \n",
      "                        \u001b[1m)\u001b[0m                                                  \n",
      "\n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m           \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m        \u001b[1m[\u001b[0m                      \u001b[32mOK\u001b[0m    \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'What is \u001b[0m      \u001b[33mmessage\u001b[0m=\u001b[32m'The \u001b[0m         \u001b[1;35mAssertionEval\u001b[0m\u001b[1m(\u001b[0m           \n",
      " \u001b[32mthe capital of \u001b[0m        \u001b[32mcapital of Poland is\u001b[0m          \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,         \n",
      " \u001b[32mPoland?'\u001b[0m               \u001b[32mWarsaw.'\u001b[0m,                     \u001b[33meval\u001b[0m=\u001b[32m'\"Warsa\u001b[0m        \n",
      " \u001b[1m)\u001b[0m                          \u001b[33mstats\u001b[0m=\u001b[1;35mResponseS\u001b[0m  \u001b[32min response.message'\u001b[0m         \n",
      "                                \u001b[33mtokens_requ\u001b[0m      \u001b[1m)\u001b[0m                        \n",
      "                                \u001b[33mtokens_resp\u001b[0m  \u001b[1m]\u001b[0m                            \n",
      "                                \u001b[33mtokens_tota\u001b[0m                               \n",
      "                                \u001b[33mconn_start_\u001b[0m                               \n",
      "                                \u001b[33mconn_end_ti\u001b[0m                               \n",
      "                                \u001b[33mconn_durati\u001b[0m                               \n",
      "                            \u001b[1m)\u001b[0m,                                             \n",
      "                            \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AK\u001b[0m                               \n",
      "                            \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                      \n",
      "                                \u001b[1m{\u001b[0m                                          \n",
      "                                    \u001b[32m'finish\u001b[0m                               \n",
      "                        \u001b[32m'stop'\u001b[0m,                                            \n",
      "                                    \u001b[32m'index'\u001b[0m:                               \n",
      "                        \u001b[1;36m0\u001b[0m,                                                 \n",
      "                                    \u001b[32m'logpro\u001b[0m                               \n",
      "                        \u001b[3;35mNone\u001b[0m,                                              \n",
      "                                    \u001b[32m'messag\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                        \u001b[32m'co\u001b[0m                               \n",
      "                        \u001b[32m'The capital of \u001b[0m                                   \n",
      "                        \u001b[32mPoland is Warsaw.'\u001b[0m,                                \n",
      "                                        \u001b[32m're\u001b[0m                               \n",
      "                        \u001b[3;35mNone\u001b[0m,                                              \n",
      "                                        \u001b[32m'ro\u001b[0m                               \n",
      "                        \u001b[32m'assistant'\u001b[0m                                        \n",
      "                                    \u001b[1m}\u001b[0m                                      \n",
      "                                \u001b[1m}\u001b[0m                                          \n",
      "                            \u001b[1m]\u001b[0m,                                             \n",
      "                            \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511\u001b[0m                               \n",
      "                            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-m\u001b[0m                               \n",
      "                            \u001b[33mobject\u001b[0m=\u001b[32m'chat.co\u001b[0m                               \n",
      "                            \u001b[33msystem_fingerpr\u001b[0m                               \n",
      "                            \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                        \n",
      "                                \u001b[32m'completion\u001b[0m                               \n",
      "                        \u001b[1;36m7\u001b[0m,                                                 \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                               \n",
      "                        \u001b[1;36m14\u001b[0m,                                                \n",
      "                                \u001b[32m'total_toke\u001b[0m                               \n",
      "                        \u001b[1;36m21\u001b[0m,                                                \n",
      "                                \u001b[32m'completion\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                    \u001b[32m'reason\u001b[0m                               \n",
      "                        \u001b[1;36m0\u001b[0m                                                  \n",
      "                                \u001b[1m}\u001b[0m,                                         \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                    \u001b[32m'cached\u001b[0m                               \n",
      "                        \u001b[1;36m0\u001b[0m                                                  \n",
      "                                \u001b[1m}\u001b[0m                                          \n",
      "                            \u001b[1m}\u001b[0m,                                             \n",
      "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpoint\u001b[0m                               \n",
      "                                \u001b[33mkind\u001b[0m=\u001b[32m'opena\u001b[0m                               \n",
      "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                    \n",
      "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-\u001b[0m                               \n",
      "                                \u001b[33madditional_\u001b[0m                               \n",
      "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNo\u001b[0m                               \n",
      "                                \u001b[33mtemperature\u001b[0m                               \n",
      "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35m\u001b[0m                               \n",
      "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                   \n",
      "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92m\u001b[0m                               \n",
      "                                \u001b[33mcollection_\u001b[0m                               \n",
      "                            \u001b[1m)\u001b[0m                                              \n",
      "                        \u001b[1m)\u001b[0m                                                  \n",
      "\n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m           \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m        \u001b[1m[\u001b[0m                      \u001b[31mFAIL\u001b[0m  \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'Please \u001b[0m       \u001b[33mmessage\u001b[0m=\u001b[32m\"In\u001b[0m\u001b[32m \u001b[0m          \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m            \n",
      " \u001b[32mwrite a 5 line poem \u001b[0m   \u001b[32mcircuits spun, where\u001b[0m          \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,         \n",
      " \u001b[32mabout AI.'\u001b[0m             \u001b[32mthoughts take \u001b[0m                \u001b[33mllm_metric\u001b[0m=\u001b[32m'\u001b[0m        \n",
      " \u001b[1m)\u001b[0m                      \u001b[32mflight,  \\nA \u001b[0m                 \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,        \n",
      "                        \u001b[32msymphony of code \u001b[0m             \u001b[33massertion\u001b[0m=\u001b[32m'T\u001b[0m        \n",
      "                        \u001b[32mpaints day and \u001b[0m       \u001b[32mshould be a poem \u001b[0m            \n",
      "                        \u001b[32mnight.  \\nWith \u001b[0m       \u001b[32mabout AI.'\u001b[0m,                  \n",
      "                        \u001b[32mwisdom forged in \u001b[0m             \u001b[33mmetric_evalu\u001b[0m        \n",
      "                        \u001b[32mdata's embrace,  \\nA\u001b[0m              \u001b[33meval_end\u001b[0m        \n",
      "                        \u001b[32mmirror of minds in \u001b[0m                   \u001b[33mconn\u001b[0m        \n",
      "                        \u001b[32mcyberspace.  \u001b[0m                             \u001b[33m\u001b[0m        \n",
      "                        \u001b[32m\\nTogether we dream,\u001b[0m                              \n",
      "                        \u001b[32min a digital light. \u001b[0m                              \n",
      "                        \u001b[32m\"\u001b[0m,                                                \n",
      "                            \u001b[33mstats\u001b[0m=\u001b[1;35mResponseS\u001b[0m                      \u001b[1m\u001b[0m        \n",
      "                                \u001b[33mtokens_requ\u001b[0m                      \u001b[33m\u001b[0m        \n",
      "                                \u001b[33mtokens_resp\u001b[0m                  \u001b[1m)\u001b[0m,           \n",
      "                                \u001b[33mtokens_tota\u001b[0m                  \u001b[33mconf\u001b[0m        \n",
      "                                \u001b[33mconn_start_\u001b[0m                      \u001b[33m\u001b[0m        \n",
      "                                \u001b[33mconn_end_ti\u001b[0m                      \u001b[33m\u001b[0m        \n",
      "                                \u001b[33mconn_durati\u001b[0m                      \u001b[33m\u001b[0m        \n",
      "                            \u001b[1m)\u001b[0m,                                    \u001b[33m\u001b[0m        \n",
      "                            \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AK\u001b[0m                      \u001b[33m\u001b[0m        \n",
      "                            \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                             \u001b[33m\u001b[0m        \n",
      "                                \u001b[1m{\u001b[0m                                 \u001b[33m\u001b[0m        \n",
      "                                    \u001b[32m'finish\u001b[0m                      \u001b[33m\u001b[0m        \n",
      "                        \u001b[32m'stop'\u001b[0m,                                   \u001b[33m\u001b[0m        \n",
      "                                    \u001b[32m'index'\u001b[0m:                      \u001b[33m\u001b[0m        \n",
      "                        \u001b[1;36m0\u001b[0m,                                    \u001b[1m)\u001b[0m            \n",
      "                                    \u001b[32m'logpro\u001b[0m              \u001b[1m)\u001b[0m,               \n",
      "                        \u001b[3;35mNone\u001b[0m,                             \u001b[33mmetric\u001b[0m=\u001b[1;35mM\u001b[0m        \n",
      "                                    \u001b[32m'messag\u001b[0m                  \u001b[33mprom\u001b[0m        \n",
      "                        \u001b[1m{\u001b[0m                     \u001b[32mare grading output \u001b[0m          \n",
      "                                        \u001b[32m'co\u001b[0m  \u001b[32maccording to a \u001b[0m              \n",
      "                        \u001b[32m\"In circuits spun, \u001b[0m   \u001b[32muser-specified \u001b[0m              \n",
      "                        \u001b[32mwhere thoughts take \u001b[0m  \u001b[32mrubric. If the \u001b[0m              \n",
      "                        \u001b[32mflight,  \\nA \u001b[0m         \u001b[32mstatement in the \u001b[0m            \n",
      "                        \u001b[32msymphony of code \u001b[0m     \u001b[32mrubric is true, then \u001b[0m        \n",
      "                        \u001b[32mpaints day and \u001b[0m       \u001b[32mthe output passes the\u001b[0m        \n",
      "                        \u001b[32mnight.  \\nWith \u001b[0m       \u001b[32mtest.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\\u001b[0m        \n",
      "                        \u001b[32mwisdom forged in \u001b[0m     \u001b[32mHello \u001b[0m                       \n",
      "                        \u001b[32mdata's embrace,  \\nA\u001b[0m  \u001b[32mworld\\n************\\\u001b[0m        \n",
      "                        \u001b[32mmirror of minds in \u001b[0m   \u001b[32mContent contains a \u001b[0m          \n",
      "                        \u001b[32mcyberspace.  \u001b[0m         \u001b[32mgreeting\\n**********\u001b[0m        \n",
      "                        \u001b[32m\\nTogether we dream,\u001b[0m  \u001b[32mAvast ye swabs, repel\u001b[0m        \n",
      "                        \u001b[32min a digital light. \u001b[0m  \u001b[32mthe \u001b[0m                         \n",
      "                        \u001b[32m\"\u001b[0m,                    \u001b[32minvaders!\\n*********\u001b[0m        \n",
      "                                        \u001b[32m're\u001b[0m  \u001b[32mDoes not speak like a\u001b[0m        \n",
      "                        \u001b[3;35mNone\u001b[0m,                 \u001b[32mpirate\\n************\u001b[0m        \n",
      "                                        \u001b[32m'ro\u001b[0m  \u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN \u001b[0m         \n",
      "                        \u001b[32m'assistant'\u001b[0m           \u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m            \n",
      "                                    \u001b[1m}\u001b[0m         \u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n**********\u001b[0m        \n",
      "                                \u001b[1m}\u001b[0m             \u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n*******\u001b[0m        \n",
      "                            \u001b[1m]\u001b[0m,                \u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour \u001b[0m               \n",
      "                            \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511\u001b[0m  \u001b[32mresponse must be a \u001b[0m          \n",
      "                            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-m\u001b[0m  \u001b[32msingle word, either \u001b[0m         \n",
      "                            \u001b[33mobject\u001b[0m=\u001b[32m'chat.co\u001b[0m  \u001b[32m\"correct\" or \u001b[0m                \n",
      "                            \u001b[33msystem_fingerpr\u001b[0m  \u001b[32m\"incorrect\", and \u001b[0m            \n",
      "                            \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m           \u001b[32mshould not contain \u001b[0m          \n",
      "                                \u001b[32m'completion\u001b[0m  \u001b[32many text or \u001b[0m                 \n",
      "                        \u001b[1;36m49\u001b[0m,                   \u001b[32mcharacters aside from\u001b[0m        \n",
      "                                \u001b[32m'prompt_tok\u001b[0m  \u001b[32mthat word.\\n\"correct\"\u001b[0m        \n",
      "                        \u001b[1;36m17\u001b[0m,                   \u001b[32mmeans that the output\u001b[0m        \n",
      "                                \u001b[32m'total_toke\u001b[0m  \u001b[32mmeets the criteria \u001b[0m          \n",
      "                        \u001b[1;36m66\u001b[0m,                   \u001b[32mspecified in the \u001b[0m            \n",
      "                                \u001b[32m'completion\u001b[0m  \u001b[32mrubric.\\n\"incorrect\" \u001b[0m        \n",
      "                        \u001b[1m{\u001b[0m                     \u001b[32mmeans that the output\u001b[0m        \n",
      "                                    \u001b[32m'reason\u001b[0m  \u001b[32mdoes not meet the \u001b[0m           \n",
      "                        \u001b[1;36m0\u001b[0m                     \u001b[32mcriteria specified in\u001b[0m        \n",
      "                                \u001b[1m}\u001b[0m,            \u001b[32mthe rubric.\\n'\u001b[0m,              \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                  \u001b[33mrail\u001b[0m        \n",
      "                        \u001b[1m{\u001b[0m                                         \u001b[32m\u001b[0m        \n",
      "                                    \u001b[32m'cached\u001b[0m  \u001b[3;92mTrue\u001b[0m,                        \n",
      "                        \u001b[1;36m0\u001b[0m                                         \u001b[32m\u001b[0m        \n",
      "                                \u001b[1m}\u001b[0m             \u001b[3;91mFalse\u001b[0m                        \n",
      "                            \u001b[1m}\u001b[0m,                                \u001b[1m}\u001b[0m            \n",
      "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpoint\u001b[0m              \u001b[1m)\u001b[0m                \n",
      "                                \u001b[33mkind\u001b[0m=\u001b[32m'opena\u001b[0m          \u001b[1m)\u001b[0m                    \n",
      "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,           \u001b[1m)\u001b[0m,                       \n",
      "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-\u001b[0m      \u001b[1;35mAssertionLLM\u001b[0m\u001b[1m(\u001b[0m            \n",
      "                                \u001b[33madditional_\u001b[0m          \u001b[33mresult\u001b[0m=\u001b[3;91mFalse\u001b[0m,        \n",
      "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNo\u001b[0m          \u001b[33mllm_metric\u001b[0m=\u001b[32m'\u001b[0m        \n",
      "                                \u001b[33mtemperature\u001b[0m          \u001b[33mreference\u001b[0m=\u001b[32m''\u001b[0m,        \n",
      "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35m\u001b[0m          \u001b[33massertion\u001b[0m=\u001b[32m'T\u001b[0m        \n",
      "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,      \u001b[32mshould be a report on\u001b[0m        \n",
      "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92m\u001b[0m  \u001b[32mtaxes.'\u001b[0m,                     \n",
      "                                \u001b[33mcollection_\u001b[0m          \u001b[33mmetric_evalu\u001b[0m        \n",
      "                            \u001b[1m)\u001b[0m                             \u001b[33meval_end\u001b[0m        \n",
      "                        \u001b[1m)\u001b[0m                                     \u001b[33mconn\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                          \n",
      "                                                                          \n",
      "                                                                          \n",
      "                                                                  \u001b[1m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                              \u001b[1m)\u001b[0m,           \n",
      "                                                              \u001b[33mconf\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                                  \u001b[33m\u001b[0m        \n",
      "                                                              \u001b[1m)\u001b[0m            \n",
      "                                                          \u001b[1m)\u001b[0m,               \n",
      "                                                          \u001b[33mmetric\u001b[0m=\u001b[1;35mM\u001b[0m        \n",
      "                                                              \u001b[33mprom\u001b[0m        \n",
      "                                              \u001b[32mare grading output \u001b[0m          \n",
      "                                              \u001b[32maccording to a \u001b[0m              \n",
      "                                              \u001b[32muser-specified \u001b[0m              \n",
      "                                              \u001b[32mrubric. If the \u001b[0m              \n",
      "                                              \u001b[32mstatement in the \u001b[0m            \n",
      "                                              \u001b[32mrubric is true, then \u001b[0m        \n",
      "                                              \u001b[32mthe output passes the\u001b[0m        \n",
      "                                              \u001b[32mtest.\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\\u001b[0m        \n",
      "                                              \u001b[32mHello \u001b[0m                       \n",
      "                                              \u001b[32mworld\\n************\\\u001b[0m        \n",
      "                                              \u001b[32mContent contains a \u001b[0m          \n",
      "                                              \u001b[32mgreeting\\n**********\u001b[0m        \n",
      "                                              \u001b[32mAvast ye swabs, repel\u001b[0m        \n",
      "                                              \u001b[32mthe \u001b[0m                         \n",
      "                                              \u001b[32minvaders!\\n*********\u001b[0m        \n",
      "                                              \u001b[32mDoes not speak like a\u001b[0m        \n",
      "                                              \u001b[32mpirate\\n************\u001b[0m        \n",
      "                                              \u001b[32mEXAMPLES\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mBEGIN \u001b[0m         \n",
      "                                              \u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mOutput\u001b[0m\u001b[32m]\u001b[0m\u001b[32m: \u001b[0m            \n",
      "                                              \u001b[32m{\u001b[0m\u001b[32moutput\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n**********\u001b[0m        \n",
      "                                              \u001b[32m{\u001b[0m\u001b[32massertion\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n*******\u001b[0m        \n",
      "                                              \u001b[32mDATA\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\n\\nYour \u001b[0m               \n",
      "                                              \u001b[32mresponse must be a \u001b[0m          \n",
      "                                              \u001b[32msingle word, either \u001b[0m         \n",
      "                                              \u001b[32m\"correct\" or \u001b[0m                \n",
      "                                              \u001b[32m\"incorrect\", and \u001b[0m            \n",
      "                                              \u001b[32mshould not contain \u001b[0m          \n",
      "                                              \u001b[32many text or \u001b[0m                 \n",
      "                                              \u001b[32mcharacters aside from\u001b[0m        \n",
      "                                              \u001b[32mthat word.\\n\"correct\"\u001b[0m        \n",
      "                                              \u001b[32mmeans that the output\u001b[0m        \n",
      "                                              \u001b[32mmeets the criteria \u001b[0m          \n",
      "                                              \u001b[32mspecified in the \u001b[0m            \n",
      "                                              \u001b[32mrubric.\\n\"incorrect\" \u001b[0m        \n",
      "                                              \u001b[32mmeans that the output\u001b[0m        \n",
      "                                              \u001b[32mdoes not meet the \u001b[0m           \n",
      "                                              \u001b[32mcriteria specified in\u001b[0m        \n",
      "                                              \u001b[32mthe rubric.\\n'\u001b[0m,              \n",
      "                                                              \u001b[33mrail\u001b[0m        \n",
      "                                                                  \u001b[32m\u001b[0m        \n",
      "                                              \u001b[3;92mTrue\u001b[0m,                        \n",
      "                                                                  \u001b[32m\u001b[0m        \n",
      "                                              \u001b[3;91mFalse\u001b[0m                        \n",
      "                                                              \u001b[1m}\u001b[0m            \n",
      "                                                          \u001b[1m)\u001b[0m                \n",
      "                                                      \u001b[1m)\u001b[0m                    \n",
      "                                                  \u001b[1m)\u001b[0m                        \n",
      "                                              \u001b[1m]\u001b[0m                            \n",
      "\n",
      " \u001b[1;35mRequestBase\u001b[0m\u001b[1m(\u001b[0m           \u001b[1;35mResponseModel\u001b[0m\u001b[1m(\u001b[0m        \u001b[1m[\u001b[0m                      \u001b[32mOK\u001b[0m    \n",
      "     \u001b[33mmessage\u001b[0m=\u001b[32m'What is \u001b[0m      \u001b[33mmessage\u001b[0m=\u001b[32m'The \u001b[0m         \u001b[1;35mAssertionDetermi\u001b[0m        \n",
      " \u001b[32mthe capital of \u001b[0m        \u001b[32mcapital of France is\u001b[0m          \u001b[33mresult\u001b[0m=\u001b[3;92mTrue\u001b[0m,         \n",
      " \u001b[32mFrance?'\u001b[0m               \u001b[32mParis.'\u001b[0m,                      \u001b[33mkind\u001b[0m=\u001b[32m'contai\u001b[0m        \n",
      " \u001b[1m)\u001b[0m                          \u001b[33mstats\u001b[0m=\u001b[1;35mResponseS\u001b[0m          \u001b[33massertion\u001b[0m=\u001b[32m'P\u001b[0m        \n",
      "                                \u001b[33mtokens_requ\u001b[0m      \u001b[1m)\u001b[0m                        \n",
      "                                \u001b[33mtokens_resp\u001b[0m  \u001b[1m]\u001b[0m                            \n",
      "                                \u001b[33mtokens_tota\u001b[0m                               \n",
      "                                \u001b[33mconn_start_\u001b[0m                               \n",
      "                                \u001b[33mconn_end_ti\u001b[0m                               \n",
      "                                \u001b[33mconn_durati\u001b[0m                               \n",
      "                            \u001b[1m)\u001b[0m,                                             \n",
      "                            \u001b[33mid\u001b[0m=\u001b[32m'chatcmpl-AK\u001b[0m                               \n",
      "                            \u001b[33mchoices\u001b[0m=\u001b[1m[\u001b[0m                                      \n",
      "                                \u001b[1m{\u001b[0m                                          \n",
      "                                    \u001b[32m'finish\u001b[0m                               \n",
      "                        \u001b[32m'stop'\u001b[0m,                                            \n",
      "                                    \u001b[32m'index'\u001b[0m:                               \n",
      "                        \u001b[1;36m0\u001b[0m,                                                 \n",
      "                                    \u001b[32m'logpro\u001b[0m                               \n",
      "                        \u001b[3;35mNone\u001b[0m,                                              \n",
      "                                    \u001b[32m'messag\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                        \u001b[32m'co\u001b[0m                               \n",
      "                        \u001b[32m'The capital of \u001b[0m                                   \n",
      "                        \u001b[32mFrance is Paris.'\u001b[0m,                                 \n",
      "                                        \u001b[32m're\u001b[0m                               \n",
      "                        \u001b[3;35mNone\u001b[0m,                                              \n",
      "                                        \u001b[32m'ro\u001b[0m                               \n",
      "                        \u001b[32m'assistant'\u001b[0m                                        \n",
      "                                    \u001b[1m}\u001b[0m                                      \n",
      "                                \u001b[1m}\u001b[0m                                          \n",
      "                            \u001b[1m]\u001b[0m,                                             \n",
      "                            \u001b[33mcreated\u001b[0m=\u001b[1;36m1729511\u001b[0m                               \n",
      "                            \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-4o-m\u001b[0m                               \n",
      "                            \u001b[33mobject\u001b[0m=\u001b[32m'chat.co\u001b[0m                               \n",
      "                            \u001b[33msystem_fingerpr\u001b[0m                               \n",
      "                            \u001b[33musage\u001b[0m=\u001b[1m{\u001b[0m                                        \n",
      "                                \u001b[32m'completion\u001b[0m                               \n",
      "                        \u001b[1;36m7\u001b[0m,                                                 \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                               \n",
      "                        \u001b[1;36m14\u001b[0m,                                                \n",
      "                                \u001b[32m'total_toke\u001b[0m                               \n",
      "                        \u001b[1;36m21\u001b[0m,                                                \n",
      "                                \u001b[32m'completion\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                    \u001b[32m'reason\u001b[0m                               \n",
      "                        \u001b[1;36m0\u001b[0m                                                  \n",
      "                                \u001b[1m}\u001b[0m,                                         \n",
      "                                \u001b[32m'prompt_tok\u001b[0m                               \n",
      "                        \u001b[1m{\u001b[0m                                                  \n",
      "                                    \u001b[32m'cached\u001b[0m                               \n",
      "                        \u001b[1;36m0\u001b[0m                                                  \n",
      "                                \u001b[1m}\u001b[0m                                          \n",
      "                            \u001b[1m}\u001b[0m,                                             \n",
      "                            \u001b[33mconfig\u001b[0m=\u001b[1;35mEndpoint\u001b[0m                               \n",
      "                                \u001b[33mkind\u001b[0m=\u001b[32m'opena\u001b[0m                               \n",
      "                                \u001b[33murl\u001b[0m=\u001b[32m''\u001b[0m,                                    \n",
      "                                \u001b[33mmodel\u001b[0m=\u001b[32m'gpt-\u001b[0m                               \n",
      "                                \u001b[33madditional_\u001b[0m                               \n",
      "                                \u001b[33mprovider\u001b[0m=\u001b[3;35mNo\u001b[0m                               \n",
      "                                \u001b[33mtemperature\u001b[0m                               \n",
      "                                \u001b[33mmax_tokens\u001b[0m=\u001b[3;35m\u001b[0m                               \n",
      "                                \u001b[33mtop_k\u001b[0m=\u001b[1;36m3\u001b[0m,                                   \n",
      "                                \u001b[33muse_ranker\u001b[0m=\u001b[3;92m\u001b[0m                               \n",
      "                                \u001b[33mcollection_\u001b[0m                               \n",
      "                            \u001b[1m)\u001b[0m                                              \n",
      "                        \u001b[1m)\u001b[0m                                                  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also run contextcheck in a command line\n",
    "!ccheck --output-type console --filename ../tests/scenario_example1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
